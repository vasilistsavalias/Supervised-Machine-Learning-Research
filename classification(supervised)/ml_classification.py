# -*- coding: utf-8 -*-
"""ml-Classification.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1BYpJdQXpwBBvYQwaXuJXrz2kGBAdjovb
"""

# first steps of the analysis.
import pandas as pd
from google.colab import drive
# Mount Google Drive & get path
drive.mount('/content/drive')
dataset_path = '/content/drive/MyDrive/ML-1/Dataset2Use_Assignment1.xlsx'
data = pd.read_excel(dataset_path)

# Display the first rows  to understand our structure
data.head()

# Checking for missing values in the dataset
missing_values = data.isnull().sum()

# Display the columns with missing values and their counts
missing_values[missing_values > 0]

from sklearn.preprocessing import MinMaxScaler

# Excluding the 'ΕΤΟΣ' and 'ΕΝΔΕΙΞΗ ΑΣΥΝΕΠΕΙΑΣ (=2) (ν+1)' columns from normalization
columns_to_normalize = data.columns.drop(['ΕΝΔΕΙΞΗ ΑΣΥΝΕΠΕΙΑΣ (=2) (ν+1)', 'ΕΤΟΣ'])

# Applying Min-Max Scaling
scaler = MinMaxScaler()
data[columns_to_normalize] = scaler.fit_transform(data[columns_to_normalize])

# Displaying the first few rows of the normalized data
data.head()

import matplotlib.pyplot as plt
import seaborn as sns

# Setting the style for the plots
sns.set(style="whitegrid")

# Graph 1: Number of healthy vs. bankrupt companies per year
# Healthy companies are indicated by 1, and bankrupt companies by 2 in the 'ΕΝΔΕΙΞΗ ΑΣΥΝΕΠΕΙΑΣ (=2) (ν+1)' column

# Grouping the data by year and company status
yearly_counts = data.groupby(['ΕΤΟΣ', 'ΕΝΔΕΙΞΗ ΑΣΥΝΕΠΕΙΑΣ (=2) (ν+1)']).size().unstack(fill_value=0)
yearly_counts.columns = ['Healthy', 'Bankrupt']  # Renaming columns for clarity

# Plotting
plt.figure(figsize=(10, 6))
yearly_counts.plot(kind='bar', stacked=False, color=['green', 'red'], ax=plt.gca())
plt.title('Number of Healthy vs. Bankrupt Companies per Year')
plt.xlabel('Year')
plt.ylabel('Number of Companies')
plt.xticks(rotation=45)
plt.legend(title='Company Status')
plt.tight_layout()
plt.show()

# Creating graphs for min, max, and average values of each indicator, separated for healthy and bankrupt companies

# Filtering data for healthy and bankrupt companies
healthy_companies = data[data['ΕΝΔΕΙΞΗ ΑΣΥΝΕΠΕΙΑΣ (=2) (ν+1)'] == 1]
bankrupt_companies = data[data['ΕΝΔΕΙΞΗ ΑΣΥΝΕΠΕΙΑΣ (=2) (ν+1)'] == 2]

# Calculating min, max, and average for each group
stats_healthy = healthy_companies[columns_to_normalize].agg(['min', 'max', 'mean'])
stats_bankrupt = bankrupt_companies[columns_to_normalize].agg(['min', 'max', 'mean'])

# Plotting
fig, axes = plt.subplots(nrows=len(columns_to_normalize), ncols=2, figsize=(15, 5 * len(columns_to_normalize)), constrained_layout=True)
fig.suptitle('Min, Max, and Average Values for Each Indicator', fontsize=16)

for i, col in enumerate(columns_to_normalize):
    # Healthy Companies
    axes[i, 0].bar(['Min', 'Max', 'Mean'], stats_healthy[col], color='green')
    axes[i, 0].set_title(f'Healthy Companies: {col}')
    axes[i, 0].set_ylim(0, max(stats_healthy[col].max(), stats_bankrupt[col].max()) * 1.1)

    # Bankrupt Companies
    axes[i, 1].bar(['Min', 'Max', 'Mean'], stats_bankrupt[col], color='red')
    axes[i, 1].set_title(f'Bankrupt Companies: {col}')
    axes[i, 1].set_ylim(0, max(stats_healthy[col].max(), stats_bankrupt[col].max()) * 1.1)

plt.show()

from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, roc_auc_score
import numpy as np

# Preparing the data for modeling
X = data[columns_to_normalize]  # Features
y = data['ΕΝΔΕΙΞΗ ΑΣΥΝΕΠΕΙΑΣ (=2) (ν+1)']  # Target variable

# Adjusting the target variable to binary (0 for healthy, 1 for bankrupt)
y = y.map({1: 0, 2: 1})

# Setting up Stratified K-Fold
skf = StratifiedKFold(n_splits=4)

# This function will be used to evaluate each model
def evaluate_model(model, X, y, kfolds):
    accuracies = []
    precisions = []
    recalls = []
    f1_scores = []
    auc_scores = []

    for train_index, test_index in kfolds.split(X, y):
        X_train, X_test = X.iloc[train_index], X.iloc[test_index]
        y_train, y_test = y.iloc[train_index], y.iloc[test_index]

        # Fit model
        model.fit(X_train, y_train)
        y_pred = model.predict(X_test)

        # Evaluate model
        accuracies.append(accuracy_score(y_test, y_pred))
        precisions.append(precision_score(y_test, y_pred))
        recalls.append(recall_score(y_test, y_pred))
        f1_scores.append(f1_score(y_test, y_pred))
        auc_scores.append(roc_auc_score(y_test, y_pred))

    # Calculating mean scores
    scores = {
        'Accuracy': np.mean(accuracies),
        'Precision': np.mean(precisions),
        'Recall': np.mean(recalls),
        'F1 Score': np.mean(f1_scores),
        'AUC ROC': np.mean(auc_scores)
    }

    return scores

# Placeholder to store the results of each model
model_results = {}

# Confirmation before proceeding to model training and evaluation
"Data preparation complete. Ready to proceed with model implementation and evaluation."



from sklearn.discriminant_analysis import LinearDiscriminantAnalysis

# Initializing the Linear Discriminant Analysis model
lda_model = LinearDiscriminantAnalysis()

# Evaluating the LDA model
lda_scores = evaluate_model(lda_model, X, y, skf)

# Storing the results
model_results['Linear Discriminant Analysis'] = lda_scores

lda_scores

from sklearn.linear_model import LogisticRegression

# Logistic Regression
log_reg_model = LogisticRegression()

# Evaluating the Logistic Regression model
log_reg_scores = evaluate_model(log_reg_model, X, y, skf)

# Storing the results
model_results['Logistic Regression'] = log_reg_scores

log_reg_scores

from sklearn.tree import DecisionTreeClassifier

# Decision Tree Classifier
dt_model = DecisionTreeClassifier()

# Evaluating the Decision Tree model
dt_scores = evaluate_model(dt_model, X, y, skf)

# Storing the results
model_results['Decision Tree'] = dt_scores

dt_scores

from sklearn.ensemble import RandomForestClassifier

# Random Forest Classifier
rf_model = RandomForestClassifier()

# Evaluating the Random Forest model
rf_scores = evaluate_model(rf_model, X, y, skf)

# Storing the results
model_results['Random Forest'] = rf_scores

rf_scores

from sklearn.neighbors import KNeighborsClassifier

# k-Nearest Neighbors Classifier
knn_model = KNeighborsClassifier()

# Evaluating the k-NN model
knn_scores = evaluate_model(knn_model, X, y, skf)

# Storing the results
model_results['k-Nearest Neighbors'] = knn_scores

knn_scores

from sklearn.naive_bayes import GaussianNB

# Naïve Bayes Classifier
nb_model = GaussianNB()

# Evaluating the Naïve Bayes model
nb_scores = evaluate_model(nb_model, X, y, skf)

# Storing the results
model_results['Naive Bayes'] = nb_scores

nb_scores

from sklearn.svm import SVC

# Support Vector Machine Classifier
svm_model = SVC(probability=True)  # 'probability=True' is needed to calculate AUC ROC later

# Evaluating the SVM model
svm_scores = evaluate_model(svm_model, X, y, skf)

# Storing the results
model_results['Support Vector Machine'] = svm_scores

svm_scores

import matplotlib.pyplot as plt

# Create a dictionary to store accuracy scores for each model
accuracy_scores = {}

# Loop through each model in model_results
for model_name, model_scores in model_results.items():
    accuracy_scores[model_name] = model_scores['Accuracy']

# Create a bar plot for accuracy scores
plt.figure(figsize=(10, 6))
plt.bar(accuracy_scores.keys(), accuracy_scores.values())
plt.title('Accuracy Scores for Different Models')
plt.xlabel('Model')
plt.ylabel('Accuracy')
plt.xticks(rotation=45)
plt.show()

# Create a dictionary to store precision, recall, and F1 scores for each model
precision_scores = {}
recall_scores = {}
f1_scores = {}

# Loop through each model in model_results
for model_name, model_scores in model_results.items():
    precision_scores[model_name] = model_scores['Precision']
    recall_scores[model_name] = model_scores['Recall']
    f1_scores[model_name] = model_scores['F1 Score']

# Create subplots for precision, recall, and F1 scores
fig, axes = plt.subplots(3, 1, figsize=(10, 18))

# Precision scores
axes[0].bar(precision_scores.keys(), precision_scores.values(), color='blue')
axes[0].set_title('Precision Scores for Different Models')
axes[0].set_ylabel('Precision')
axes[0].tick_params(axis='x', rotation=45)

# Recall scores
axes[1].bar(recall_scores.keys(), recall_scores.values(), color='green')
axes[1].set_title('Recall Scores for Different Models')
axes[1].set_ylabel('Recall')
axes[1].tick_params(axis='x', rotation=45)

# F1 scores
axes[2].bar(f1_scores.keys(), f1_scores.values(), color='purple')
axes[2].set_title('F1 Scores for Different Models')
axes[2].set_ylabel('F1 Score')
axes[2].tick_params(axis='x', rotation=45)

plt.tight_layout()
plt.show()






# Create a dictionary to store AUC ROC scores for each model
auc_roc_scores = {}

# Loop through each model in model_results
for model_name, model_scores in model_results.items():
    auc_roc_scores[model_name] = model_scores['AUC ROC']

# Create a bar plot for AUC ROC scores
plt.figure(figsize=(10, 6))
plt.bar(auc_roc_scores.keys(), auc_roc_scores.values(), color='orange')
plt.title('AUC ROC Scores for Different Models')
plt.xlabel('Model')
plt.ylabel('AUC ROC Score')
plt.xticks(rotation=45)
plt.show()


# Create dictionaries to store Specificity and MCC scores for each model
specificity_scores = {}
mcc_scores = {}

# Loop through each model in model_results
for model_name, model_scores in model_results.items():
    specificity_scores[model_name] = model_scores['Specificity']
    mcc_scores[model_name] = model_scores['MCC']

# Create subplots for Specificity and MCC scores
fig, axes = plt.subplots(2, 1, figsize=(10, 12))

# Specificity scores
axes[0].bar(specificity_scores.keys(), specificity_scores.values(), color='teal')
axes[0].set_title('Specificity Scores for Different Models')
axes[0].set_ylabel('Specificity')
axes[0].tick_params(axis='x', rotation=45)

# MCC scores
axes[1].bar(mcc_scores.keys(), mcc_scores.values(), color='coral')
axes[1].set_title('MCC Scores for Different Models')
axes[1].set_ylabel('MCC')
axes[1].tick_params(axis='x', rotation=45)

plt.tight_layout()
plt.show()

import seaborn as sns
import matplotlib.pyplot as plt
import pandas as pd

# Create a DataFrame to store the metrics for each model
metrics_df = pd.DataFrame.from_dict(model_results, orient='index')

# Create a heatmap for model performance
plt.figure(figsize=(12, 8))
sns.heatmap(metrics_df, annot=True, cmap='coolwarm', fmt=".3f")
plt.title('Model Performance Heatmap')
plt.xlabel('Metrics')
plt.ylabel('Models')
plt.xticks(rotation=45)
plt.show()

import matplotlib.pyplot as plt
import pandas as pd

#F1 Score: 0.25 - Important for balancing precision and recall, especially in imbalanced datasets.
#AUC ROC: 0.25 - Provides an overall measure of the model's ability to distinguish between the classes.
#Recall: 0.2 - Critical for minimizing false negatives (failing to identify a bankrupt company).
#Specificity: 0.1 - Ensures the model is not overly pessimistic, reducing false positives (wrongly identifying a healthy company as bankrupt).
#Precision: 0.1 - Important but slightly less so than recall in this context.
#Accuracy: 0.05 - While overall accuracy is important, it can be misleading in imbalanced datasets.
#MCC: 0.05 - A comprehensive metric that considers all four quadrants of the confusion matrix.

# Define weights for each metric
weights = {
    'Accuracy': 0.05,
    'Precision': 0.1,
    'Recall': 0.2,
    'F1 Score': 0.25,
    'AUC ROC': 0.25,
    'Specificity': 0.1,
    'MCC': 0.05
}

# Calculate the composite score for each model with updated weights
composite_scores = {}
for model_name, model_scores in model_results.items():
    composite_score = sum(model_scores[metric] * weights[metric] for metric in weights)
    composite_scores[model_name] = composite_score

# Create a DataFrame for the composite scores
composite_df = pd.DataFrame.from_dict(composite_scores, orient='index', columns=['Composite Score'])

# Sort models by composite score in descending order
composite_df = composite_df.sort_values(by='Composite Score', ascending=False)

# Create a bar graph to visualize the composite scores
plt.figure(figsize=(10, 6))
bars = plt.bar(composite_df.index, composite_df['Composite Score'], color='skyblue')
plt.title('Composite Scores for Different Models')
plt.xlabel('Model')
plt.ylabel('Composite Score')
plt.xticks(rotation=45)

# Annotate the bars with their composite scores
for bar, score in zip(bars, composite_df['Composite Score']):
    plt.text(bar.get_x() + bar.get_width() / 2, bar.get_height(), f'{score:.2f}', ha='center', va='bottom')

plt.show()

import pickle
def save_metrics(model_name, metrics):
    with open(f'{model_name}_metrics.pkl', 'wb') as file:
        pickle.dump(metrics, file)
    print(f'{model_name} metrics saved!')

# Function to calculate Specificity and MCC
def calculate_specificity_mcc(y_true, y_pred):
    cm = confusion_matrix(y_true, y_pred)
    TN, FP, FN, TP = cm.ravel()
    specificity = TN / (TN + FP)
    mcc = matthews_corrcoef(y_true, y_pred)
    return specificity, mcc

#will be used later to have an approximation of time complexity of the future-to-be-tuned algorithms
durationDict = {}

import joblib
import pickle
import numpy as np
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import make_scorer, f1_score, roc_auc_score, accuracy_score, precision_score, recall_score, confusion_matrix, matthews_corrcoef
from sklearn.neighbors import KNeighborsClassifier
from datetime import datetime


start_time = datetime.now()


# K-Nearest Neighbors (KNN) Classifier Parameters
knn_param_grid = {
    'n_neighbors': [3, 5, 7, 9],
    'weights': ['uniform', 'distance'],
    'p': [1, 2]
}

# Grid Search Setup for KNN
knn_classifier = KNeighborsClassifier()
knn_grid_search = GridSearchCV(knn_classifier, knn_param_grid, cv=4, scoring=make_scorer(f1_score), n_jobs=-1)
knn_grid_search.fit(X, y)
knn_best_model = knn_grid_search.best_estimator_

# Make predictions with the best model
y_pred_knn = knn_best_model.predict(X)

# Calculate additional metrics
specificity_knn, mcc_knn = calculate_specificity_mcc(y, y_pred_knn)

# Collecting all metrics
knn_metrics = {
    'Best Parameters': knn_grid_search.best_params_,
    'Best F1 Score': knn_grid_search.best_score_,
    'Best AUC ROC': roc_auc_score(y, y_pred_knn),
    'Best Accuracy': accuracy_score(y, y_pred_knn),
    'Best Precision': precision_score(y, y_pred_knn),
    'Best Recall': recall_score(y, y_pred_knn),
    'Specificity': specificity_knn,
    'MCC': mcc_knn
}

# Print and save the results
print("K-Nearest Neighbors Grid Search Results:")
for metric, value in knn_metrics.items():
    print(f"{metric}: {value}")

save_metrics('knn', knn_metrics)
joblib.dump(knn_best_model, 'knn_model.pkl')
print('K-Nearest Neighbors (KNN) model saved!')


end_time = datetime.now()
duration = end_time - start_time

durationDict['Knn'] = duration

print(duration)

import matplotlib.pyplot as plt

# Define the metrics and their corresponding values
metrics = ['F1 Score', 'AUC ROC', 'Accuracy', 'Precision', 'Recall', 'Specificity', 'MCC']
values = [
    knn_metrics['Best F1 Score'],
    knn_metrics['Best AUC ROC'],
    knn_metrics['Best Accuracy'],
    knn_metrics['Best Precision'],
    knn_metrics['Best Recall'],
    knn_metrics['Specificity'],
    knn_metrics['MCC']
]

# Create a bar graph to display metrics
plt.figure(figsize=(10, 6))
bars = plt.bar(metrics, values, color='lightcoral')
plt.title('K-Nearest Neighbors (KNN) Model Metrics',pad=30) #the paddding is to avoid text collision if the accuracy is 1.00)
plt.ylabel('Metric Value')
plt.ylim(0, 1)  # Set y-axis limit to match the score range (0 to 1)
plt.xticks(rotation=45, ha="right")  # Rotate x-axis labels for better readability

# Annotate the bars with their values
for bar, value in zip(bars, values):
    plt.text(bar.get_x() + bar.get_width() / 2, bar.get_height(), f'{value:.2f}', ha='center', va='bottom')

plt.tight_layout()
plt.show()

from sklearn.model_selection import GridSearchCV
from sklearn.metrics import make_scorer, f1_score, roc_auc_score, accuracy_score, precision_score, recall_score, confusion_matrix, matthews_corrcoef
from sklearn.naive_bayes import GaussianNB
from datetime import datetime

# Start time for performance measurement
start_time = datetime.now()

# Naive Bayes Classifier parameters
naive_bayes_param_grid = {
    'var_smoothing': [1e-9, 1e-8, 1e-7, 1e-6, 1e-5]
}

# Grid Search Setup for Naive Bayes
nb_classifier = GaussianNB()
nb_grid_search = GridSearchCV(nb_classifier, naive_bayes_param_grid, cv=4, scoring=make_scorer(f1_score), n_jobs=-1)
nb_grid_search.fit(X, y)
nb_best_model = nb_grid_search.best_estimator_

# Make predictions with the best model
y_pred_nb = nb_best_model.predict(X)

# Calculate additional metrics
specificity_nb, mcc_nb = calculate_specificity_mcc(y, y_pred_nb)

# Collecting all metrics for Naive Bayes
nb_metrics = {
    'Best Parameters': nb_grid_search.best_params_,
    'Best F1 Score': nb_grid_search.best_score_,
    'Best AUC ROC': roc_auc_score(y, y_pred_nb),
    'Best Accuracy': accuracy_score(y, y_pred_nb),
    'Best Precision': precision_score(y, y_pred_nb),
    'Best Recall': recall_score(y, y_pred_nb),
    'Specificity': specificity_nb,
    'MCC': mcc_nb
}

# Print and save the results
print("Naive Bayes Grid Search Results:")
for metric, value in nb_metrics.items():
    print(f"{metric}: {value}")

save_metrics('naive_bayes', nb_metrics)
joblib.dump(nb_best_model, 'naive_bayes_model.pkl')
print('Naive Bayes model saved!')

# Measure the time taken and store it
end_time = datetime.now()
duration = end_time - start_time
print("Duration: ", duration)

import matplotlib.pyplot as plt

# Define the metrics and their corresponding values for Naive Bayes
metrics = ['F1 Score', 'AUC ROC', 'Accuracy', 'Precision', 'Recall', 'Specificity', 'MCC']
nb_values = [
    nb_metrics['Best F1 Score'],
    nb_metrics['Best AUC ROC'],
    nb_metrics['Best Accuracy'],
    nb_metrics['Best Precision'],
    nb_metrics['Best Recall'],
    nb_metrics['Specificity'],
    nb_metrics['MCC']
]

# Create a bar graph to display metrics for Naive Bayes
plt.figure(figsize=(10, 6))
bars = plt.bar(metrics, nb_values, color='lightcoral')
plt.title('Naive Bayes Model Metrics')
plt.ylabel('Metric Value')
plt.ylim(0, 1)  # Set y-axis limit to match the score range (0 to 1)
plt.xticks(rotation=45, ha="right")  # Rotate x-axis labels for better readability

# Annotate the bars with their values
for bar, value in zip(bars, nb_values):
    plt.text(bar.get_x() + bar.get_width() / 2, bar.get_height(), f'{value:.2f}', ha='center', va='bottom')

plt.tight_layout()
plt.show()

import joblib
import pickle
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import make_scorer, f1_score, roc_auc_score, accuracy_score, precision_score, recall_score
from sklearn.tree import DecisionTreeClassifier
from datetime import datetime


start_time = datetime.now()
# Decision Tree Classifier
dt_param_grid = {
    'max_depth': [None, 5, 10, 15,20],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4],
    'criterion': ['gini', 'entropy']
}

# Grid Search Setup for Decision Trees
dt_classifier = DecisionTreeClassifier()
dt_grid_search = GridSearchCV(dt_classifier, dt_param_grid, cv=4, scoring=make_scorer(f1_score), n_jobs=-1)
dt_grid_search.fit(X, y)
dt_best_model = dt_grid_search.best_estimator_

# Make predictions with the best model
y_pred_dt = dt_best_model.predict(X)

# Calculate additional metrics
specificity_dt, mcc_dt = calculate_specificity_mcc(y, y_pred_dt)

# Collecting all metrics for Decision Trees
dt_metrics = {
    'Best Parameters': dt_grid_search.best_params_,
    'Best F1 Score': dt_grid_search.best_score_,
    'Best AUC ROC': roc_auc_score(y, y_pred_dt),
    'Best Accuracy': accuracy_score(y, y_pred_dt),
    'Best Precision': precision_score(y, y_pred_dt),
    'Best Recall': recall_score(y, y_pred_dt),
    'Specificity': specificity_dt,
    'MCC': mcc_dt
}

# Print and save the results for Decision Trees
print("Decision Trees Grid Search Results:")
for metric, value in dt_metrics.items():
    print(f"{metric}: {value}")

save_metrics('decision_trees', dt_metrics)
joblib.dump(dt_best_model, 'decision_tree_model.pkl')
print('Decision Trees model saved!')

end_time = datetime.now()
duration = end_time - start_time

durationDict['Decision Trees'] = duration

print(duration)

import matplotlib.pyplot as plt

# Define the metrics and their corresponding values for Naive Bayes
metrics = ['F1 Score', 'AUC ROC', 'Accuracy', 'Precision', 'Recall', 'Specificity', 'MCC']
dt_values = [
    dt_metrics['Best F1 Score'],
    dt_metrics['Best AUC ROC'],
    dt_metrics['Best Accuracy'],
    dt_metrics['Best Precision'],
    dt_metrics['Best Recall'],
    dt_metrics['Specificity'],
    dt_metrics['MCC']
]

# Create a bar graph to display metrics for Naive Bayes
plt.figure(figsize=(10, 6))
bars = plt.bar(metrics, dt_values, color='lightcoral')
plt.title('Decision Trees Model Metrics',pad=30)#the paddding is to avoid text collision if the accuracy is 1.00
plt.ylabel('Metric Value')
plt.ylim(0, 1)  # Set y-axis limit to match the score range (0 to 1)
plt.xticks(rotation=45, ha="right")  # Rotate x-axis labels for better readability

# Annotate the bars with their values
for bar, value in zip(bars, dt_values):
    plt.text(bar.get_x() + bar.get_width() / 2, bar.get_height(), f'{value:.2f}', ha='center', va='bottom')

plt.tight_layout()
plt.show()

import joblib
import numpy as np
from sklearn.model_selection import GridSearchCV, train_test_split
from sklearn.metrics import make_scorer, f1_score, roc_auc_score, accuracy_score, precision_score, recall_score, confusion_matrix, matthews_corrcoef
from sklearn.ensemble import RandomForestClassifier
from datetime import datetime


start_time = datetime.now()


# Split the data into training and validation sets
X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)

# Random Forests Classifier Parameters
rf_param_grid = {
    'n_estimators': [50, 100, 150],
    'criterion': ['gini', 'entropy'],
    'max_depth': [None, 5, 10, 15, 20],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4],
}

# Grid Search Setup for Random Forests with Early Stopping
best_f1_score = 0.0
best_model = None

for n_estimators in rf_param_grid['n_estimators']:
    for criterion in rf_param_grid['criterion']:
        for max_depth in rf_param_grid['max_depth']:
            for min_samples_split in rf_param_grid['min_samples_split']:
                for min_samples_leaf in rf_param_grid['min_samples_leaf']:
                    # Create and train the Random Forest with early stopping
                    rf_classifier = RandomForestClassifier(
                        n_estimators=n_estimators,
                        criterion=criterion,
                        max_depth=max_depth,
                        min_samples_split=min_samples_split,
                        min_samples_leaf=min_samples_leaf,
                        warm_start=True,  # Enable early stopping
                        random_state=42
                    )

                    # Train the model on the training data
                    rf_classifier.fit(X_train, y_train)

                    # Make predictions on the validation set
                    y_pred_val = rf_classifier.predict(X_val)

                    # Calculate the F1 score on the validation set
                    f1 = f1_score(y_val, y_pred_val)

                    # Check if the current model has a better F1 score
                    if f1 > best_f1_score:
                        best_f1_score = f1
                        best_model = rf_classifier

# Make predictions with the best model
y_pred_rf = best_model.predict(X)

# Calculate additional metrics
specificity_rf, mcc_rf = calculate_specificity_mcc(y, y_pred_rf)

# Collecting all metrics for Random Forests
rf_metrics = {
    'Best F1 Score': best_f1_score,
    'Best AUC ROC': roc_auc_score(y, y_pred_rf),
    'Best Accuracy': accuracy_score(y, y_pred_rf),
    'Best Precision': precision_score(y, y_pred_rf),
    'Best Recall': recall_score(y, y_pred_rf),
    'Specificity': specificity_rf,
    'MCC': mcc_rf
}

# Print and save the results for Random Forests
print("Random Forests Grid Search Results:")
for metric, value in rf_metrics.items():
    print(f"{metric}: {value}")

joblib.dump(best_model, 'random_forests_model.pkl')
print('Random Forests model saved!')
end_time = datetime.now()
duration = end_time - start_time

durationDict['RF-ES'] = duration

print(duration)

save_metrics('random_forests', rf_metrics)

import joblib
import numpy as np
from sklearn.model_selection import GridSearchCV, train_test_split
from sklearn.metrics import make_scorer, f1_score, roc_auc_score, accuracy_score, precision_score, recall_score, confusion_matrix, matthews_corrcoef
from sklearn.ensemble import RandomForestClassifier
import matplotlib.pyplot as plt

# Define the metrics and their corresponding values for Naive Bayes
metrics = ['F1 Score', 'AUC ROC', 'Accuracy', 'Precision', 'Recall', 'Specificity', 'MCC']
rf_values = [
    rf_metrics['Best F1 Score'],
    rf_metrics['Best AUC ROC'],
    rf_metrics['Best Accuracy'],
    rf_metrics['Best Precision'],
    rf_metrics['Best Recall'],
    rf_metrics['Specificity'],
    rf_metrics['MCC']
]

# Create a bar graph to display metrics for Naive Bayes
plt.figure(figsize=(10, 6))
bars = plt.bar(metrics, rf_values, color='lightcoral')
plt.title('Random Forest Model Metrics',pad=30) #the paddding is to avoid text collision if the accuracy is 1.00
plt.ylabel('Metric Value')
plt.ylim(0, 1)  # Set y-axis limit to match the score range (0 to 1)
plt.xticks(rotation=45, ha="right")  # Rotate x-axis labels for better readability

# Annotate the bars with their values
for bar, value in zip(bars, rf_values):
    plt.text(bar.get_x() + bar.get_width() / 2, bar.get_height(), f'{value:.2f}', ha='center', va='bottom')

plt.tight_layout()
plt.show()

import joblib
import numpy as np
from sklearn.model_selection import GridSearchCV, train_test_split
from sklearn.metrics import make_scorer, f1_score, roc_auc_score, accuracy_score, precision_score, recall_score, confusion_matrix, matthews_corrcoef
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from datetime import datetime


start_time = datetime.now()

# Linear Discriminant Analysis (LDA) Parameters
lda_param_grid = {
    'solver': ['svd', 'lsqr', 'eigen'],
    'shrinkage': [None, 'auto'],
    'n_components': [None, 1, 2, 3]  # Adjust the number of components as needed
}

# Grid Search Setup for LDA
lda_classifier = LinearDiscriminantAnalysis()
lda_grid_search = GridSearchCV(lda_classifier, lda_param_grid, cv=4, scoring=make_scorer(f1_score), n_jobs=-1)
lda_grid_search.fit(X, y)
lda_best_model = lda_grid_search.best_estimator_

# Make predictions with the best model
y_pred_lda = lda_best_model.predict(X)

# Calculate additional metrics
specificity_lda, mcc_lda = calculate_specificity_mcc(y, y_pred_lda)

# Collecting all metrics for LDA
lda_metrics = {
    'Best Parameters': lda_grid_search.best_params_,
    'Best F1 Score': lda_grid_search.best_score_,
    'Best AUC ROC': roc_auc_score(y, y_pred_lda),
    'Best Accuracy': accuracy_score(y, y_pred_lda),
    'Best Precision': precision_score(y, y_pred_lda),
    'Best Recall': recall_score(y, y_pred_lda),
    'Specificity': specificity_lda,
    'MCC': mcc_lda
}

# Print and save the results for LDA
print("Linear Discriminant Analysis Grid Search Results:")
for metric, value in lda_metrics.items():
    print(f"{metric}: {value}")

save_metrics('lda', lda_metrics)
joblib.dump(lda_best_model, 'lda_model.pkl')
print('Linear Discriminant Analysis (LDA) model saved!')

end_time = datetime.now()
duration = end_time - start_time

durationDict['LDA'] = duration

print(duration)

import joblib
import numpy as np
from sklearn.model_selection import GridSearchCV, train_test_split
from sklearn.metrics import make_scorer, f1_score, roc_auc_score, accuracy_score, precision_score, recall_score, confusion_matrix, matthews_corrcoef
from sklearn.ensemble import RandomForestClassifier
import matplotlib.pyplot as plt

# Define the metrics and their corresponding values for Naive Bayes
metrics = ['F1 Score', 'AUC ROC', 'Accuracy', 'Precision', 'Recall', 'Specificity', 'MCC']
lda_values = [
    lda_metrics['Best F1 Score'],
    lda_metrics['Best AUC ROC'],
    lda_metrics['Best Accuracy'],
    lda_metrics['Best Precision'],
    lda_metrics['Best Recall'],
    lda_metrics['Specificity'],
    lda_metrics['MCC']
]

# Create a bar graph to display metrics for Naive Bayes
plt.figure(figsize=(10, 6))
bars = plt.bar(metrics, lda_values, color='lightcoral')
plt.title('LDA Model Metrics',pad=30) #the paddding is to avoid text collision if the accuracy is 1.00
plt.ylabel('Metric Value')
plt.ylim(0, 1)  # Set y-axis limit to match the score range (0 to 1)
plt.xticks(rotation=45, ha="right")  # Rotate x-axis labels for better readability

# Annotate the bars with their values
for bar, value in zip(bars, lda_values):
    plt.text(bar.get_x() + bar.get_width() / 2, bar.get_height(), f'{value:.2f}', ha='center', va='bottom')

plt.tight_layout()
plt.show()

from sklearn.decomposition import PCA
import matplotlib.pyplot as plt

# Separating out the features
X = data.drop(columns=['ΕΝΔΕΙΞΗ ΑΣΥΝΕΠΕΙΑΣ (=2) (ν+1)', 'ΕΤΟΣ'])  # Assuming the target column and non-feature columns are removed

# Standardizing the features
X_standardized = (X - X.mean()) / X.std()

# Performing PCA
pca = PCA().fit(X_standardized)

# Cumulative variance explained by the principal components
cumulative_variance = pca.explained_variance_ratio_.cumsum()

# Plotting the cumulative variance
plt.figure(figsize=(10, 6))
plt.plot(range(1, len(cumulative_variance) + 1), cumulative_variance, marker='o')
plt.xlabel('Number of Components')
plt.ylabel('Cumulative Explained Variance')
plt.title('Explained Variance by Different Principal Components')
plt.grid(True)
plt.show()

# Applying PCA with 10 components
pca_10 = PCA(n_components=10)
X_reduced_10 = pca_10.fit_transform(X_standardized)

# Checking the shape of the reduced dataset
X_reduced_10.shape

from sklearn.svm import SVC
from sklearn.decomposition import PCA
from sklearn.model_selection import GridSearchCV, train_test_split
from sklearn.metrics import make_scorer, f1_score, roc_auc_score, accuracy_score, precision_score, recall_score, confusion_matrix, matthews_corrcoef
import joblib
import numpy as np
from datetime import datetime

# Assume X and y are your data and labels

# Start timer
start_time = datetime.now()

# Applying PCA for dimensionality reduction
pca = pca_10
X_reduced = X_reduced_10

# Reduced SVM Parameter Grid
svm_param_grid = {
    'C': [1, 10, 100],  # Reduced number of values for C
    'gamma': [0.01, 0.001, 0.0001],  # Reduced number of values for gamma
    'kernel': ['rbf']
}

# Grid Search Setup for SVM
svm_classifier = SVC(probability=True, verbose=True, tol=1e-3)  # Increased tolerance for faster convergence
svm_grid_search = GridSearchCV(svm_classifier, svm_param_grid, cv=3, scoring=make_scorer(f1_score), n_jobs=-1)  # Reduced CV folds
svm_grid_search.fit(X_reduced, y)
svm_best_model = svm_grid_search.best_estimator_

# Make predictions with the best model
y_pred_svm = svm_best_model.predict(X_reduced)

# Additional metrics functions (assuming you have them defined)
specificity_svm, mcc_svm = calculate_specificity_mcc(y, y_pred_svm)

# Collecting all metrics for SVM
svm_metrics = {
    'Best Parameters': svm_grid_search.best_params_,
    'Best F1 Score': svm_grid_search.best_score_,
    'Best AUC ROC': roc_auc_score(y, y_pred_svm),
    'Best Accuracy': accuracy_score(y, y_pred_svm),
    'Best Precision': precision_score(y, y_pred_svm),
    'Best Recall': recall_score(y, y_pred_svm),
    'Specificity': specificity_svm,
    'MCC': mcc_svm
}

# Print and save the results for SVM
print("Support Vector Machines Grid Search Results:")
for metric, value in svm_metrics.items():
    print(f"{metric}: {value}")

# Function to save metrics (assuming you have it defined)
save_metrics('svm', svm_metrics)

# Save the best SVM model
joblib.dump(svm_best_model, 'svm_model.pkl')
print('Support Vector Machines (SVM) model saved!')

# Calculate duration
end_time = datetime.now()
duration = end_time - start_time
durationDict['SVM'] = duration

print(f"Duration: {duration}")

import joblib
import numpy as np
from sklearn.model_selection import GridSearchCV, train_test_split
from sklearn.metrics import make_scorer, f1_score, roc_auc_score, accuracy_score, precision_score, recall_score, confusion_matrix, matthews_corrcoef
import matplotlib.pyplot as plt

# Define the metrics and their corresponding values for Naive Bayes
metrics = ['F1 Score', 'AUC ROC', 'Accuracy', 'Precision', 'Recall', 'Specificity', 'MCC']
svm_values = [
    svm_metrics['Best F1 Score'],
    svm_metrics['Best AUC ROC'],
    svm_metrics['Best Accuracy'],
    svm_metrics['Best Precision'],
    svm_metrics['Best Recall'],
    svm_metrics['Specificity'],
    svm_metrics['MCC']
]

# Create a bar graph to display metrics for Naive Bayes
plt.figure(figsize=(10, 6))
bars = plt.bar(metrics, svm_values, color='lightcoral')
plt.title('SVM Model Metrics',pad=30) #the paddding is to avoid text collision if the accuracy is 1.00
plt.ylabel('Metric Value')
plt.ylim(0, 1)  # Set y-axis limit to match the score range (0 to 1)
plt.xticks(rotation=45, ha="right")  # Rotate x-axis labels for better readability

# Annotate the bars with their values
for bar, value in zip(bars, svm_values):
    plt.text(bar.get_x() + bar.get_width() / 2, bar.get_height(), f'{value:.2f}', ha='center', va='bottom')

plt.tight_layout()
plt.show()

import joblib
import numpy as np
from sklearn.model_selection import GridSearchCV, train_test_split
from sklearn.metrics import make_scorer, f1_score, roc_auc_score, accuracy_score, precision_score, recall_score, confusion_matrix, matthews_corrcoef
import matplotlib.pyplot as plt
from sklearn.linear_model import LogisticRegression
from datetime import datetime

# Assume X and y are your data and labels

# Start timer
start_time = datetime.now()

# Logistic Regression Parameters
logistic_param_grid = {
    'penalty': ['l1', 'l2'],
    'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000],  # You can adjust the C values
    'solver': ['liblinear', 'saga']
}

# Grid Search Setup for Logistic Regression
logistic_classifier = LogisticRegression(max_iter=10000)
logistic_grid_search = GridSearchCV(logistic_classifier, logistic_param_grid, cv=4, scoring=make_scorer(f1_score), n_jobs=-1)
logistic_grid_search.fit(X, y)
logistic_best_model = logistic_grid_search.best_estimator_

# Make predictions with the best model
y_pred_logistic = logistic_best_model.predict(X)

# Calculate additional metrics
specificity_logistic, mcc_logistic = calculate_specificity_mcc(y, y_pred_logistic)

# Collecting all metrics for Logistic Regression
logistic_metrics = {
    'Best Parameters': logistic_grid_search.best_params_,
    'Best F1 Score': logistic_grid_search.best_score_,
    'Best AUC ROC': roc_auc_score(y, y_pred_logistic),
    'Best Accuracy': accuracy_score(y, y_pred_logistic),
    'Best Precision': precision_score(y, y_pred_logistic),
    'Best Recall': recall_score(y, y_pred_logistic),
    'Specificity': specificity_logistic,
    'MCC': mcc_logistic
}

# Print and save the results for Logistic Regression
print("Logistic Regression Grid Search Results:")
for metric, value in logistic_metrics.items():
    print(f"{metric}: {value}")

save_metrics('logistic', logistic_metrics)
joblib.dump(logistic_best_model, 'logistic_model.pkl')
print('Logistic Regression model saved!')
end_time = datetime.now()
duration = end_time - start_time

durationDict['Logistic Reg'] = duration

print(duration)

import joblib
import numpy as np
from sklearn.model_selection import GridSearchCV, train_test_split
from sklearn.metrics import make_scorer, f1_score, roc_auc_score, accuracy_score, precision_score, recall_score, confusion_matrix, matthews_corrcoef
import matplotlib.pyplot as plt

# Define the metrics and their corresponding values for Naive Bayes
metrics = ['F1 Score', 'AUC ROC', 'Accuracy', 'Precision', 'Recall', 'Specificity', 'MCC']
logistic_values = [
    logistic_metrics['Best F1 Score'],
    logistic_metrics['Best AUC ROC'],
    logistic_metrics['Best Accuracy'],
    logistic_metrics['Best Precision'],
    logistic_metrics['Best Recall'],
    logistic_metrics['Specificity'],
    logistic_metrics['MCC']
]

# Create a bar graph to display metrics for Naive Bayes
plt.figure(figsize=(10, 6))
bars = plt.bar(metrics, logistic_values, color='lightcoral')
plt.title('Logistic Reg Model Metrics',pad=30) #the paddding is to avoid text collision if the accuracy is 1.00
plt.ylabel('Metric Value')
plt.ylim(0, 1)  # Set y-axis limit to match the score range (0 to 1)
plt.xticks(rotation=45, ha="right")  # Rotate x-axis labels for better readability

# Annotate the bars with their values
for bar, value in zip(bars, logistic_values):
    plt.text(bar.get_x() + bar.get_width() / 2, bar.get_height(), f'{value:.2f}', ha='center', va='bottom')

plt.tight_layout()
plt.show()

import matplotlib.pyplot as plt

# Time durations in seconds
durations_seconds = [duration.total_seconds() for duration in durationDict.values()]

# Model names or categories
categories = list(durationDict.keys())

# Create a list of (category, duration) tuples and sort them by duration in ascending order
sorted_categories_and_durations = sorted(zip(categories, durations_seconds), key=lambda x: x[1])

# Separate the sorted tuples back into separate lists
sorted_categories, sorted_durations_seconds = zip(*sorted_categories_and_durations)

# Convert durations to minutes and seconds with "m"
durations_formatted = []
for duration_seconds in sorted_durations_seconds:
    duration_minutes = int(duration_seconds // 60)
    duration_seconds_remaining = int(duration_seconds % 60)
    duration_formatted = f"{duration_minutes}m {duration_seconds_remaining}s"
    durations_formatted.append(duration_formatted)

# Create a bar graph with a logarithmic scale
plt.bar(sorted_categories, sorted_durations_seconds, color='saddlebrown')
plt.yscale('log')  # Use a logarithmic scale on the y-axis

# Add labels and title
plt.xlabel('Categories')
plt.ylabel('Time(s)')
plt.title('Time Duration for Different Models',pad=30)

# Annotate the bars with time durations
for i, duration in enumerate(durations_formatted):
    plt.annotate(duration, (sorted_categories[i], sorted_durations_seconds[i]), textcoords="offset points", xytext=(0, 5), ha='center')

# Rotate x-axis labels for better readability
plt.xticks(rotation=45)

# Show the plot
plt.tight_layout()  # Ensures labels are not cut off
plt.show()

import joblib
import pickle
import os
from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import make_scorer, f1_score, roc_auc_score
from sklearn.model_selection import cross_val_score
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis

# Define a function to load metrics
def load_metrics(model_name):
    try:
        with open(f'{model_name}_metrics.pkl', 'rb') as file:
            return pickle.load(file)
    except FileNotFoundError:
        return None

# Define a function to calculate the composite score
def calculate_composite_score(f1_score, auc_roc_score, f1_weight=0.7, auc_roc_weight=0.3):
    return (f1_weight * f1_score) + (auc_roc_weight * auc_roc_score)

# Define a dictionary to store the models
models = {
    "best_knn_": joblib.load('knn_model.pkl'),
    "best_naive_bayes": joblib.load('naive_bayes_model.pkl'),
    "best_decision_tree": joblib.load('decision_tree_model.pkl'),
    "best_random_forest": joblib.load('random_forests_model.pkl'),
    "best_lda": joblib.load('lda_model.pkl'),
    "best_svm": joblib.load('svm_model.pkl'),
    "best_logistic": joblib.load('logistic_model.pkl'),

}

# Define the cross-validation strategy (e.g., Stratified K-Fold)
skf = StratifiedKFold(n_splits=4, shuffle=True, random_state=42)

# Initialize variables to track the best model and its score
best_model_name = None
best_composite_score = 0.0  # Initialize with a low score
best_metrics = None  # Initialize with None

# Iterate through models and calculate their composite scores
for model_name, model in models.items():
    # Calculate the mean F1 Score using cross-validation
    f1_scores = cross_val_score(model, X, y, cv=skf, scoring=make_scorer(f1_score))
    mean_f1_score = f1_scores.mean()

    # Calculate the mean AUC ROC Score using cross-validation
    auc_roc_scores = cross_val_score(model, X, y, cv=skf, scoring=make_scorer(roc_auc_score))
    mean_auc_roc_score = auc_roc_scores.mean()

    # Calculate the composite score
    composite_score = calculate_composite_score(mean_f1_score, mean_auc_roc_score)

    # Check if the current model has a higher composite score
    if composite_score > best_composite_score:
        best_composite_score = composite_score
        best_model_name = model_name

# Print the best-performing model and its composite score
if best_model_name is not None:
    print("Best Model:", best_model_name)
    print("Best Composite Score:", best_composite_score)

    # Load the best-performing model
    best_model = models.get(best_model_name)
    if best_model is not None:
        print(f"Loaded the best model: {best_model_name}")


        # Remove the "best_" prefix from best_model_name and keep the model name
        model_name_without_prefix = best_model_name.replace("best_", "")

        # Remove the "_model" suffix from best_model_name and keep the model name
        model_name_without_prefix_without_suffix = model_name_without_prefix.replace("_model", "")




        # Duplicate and rename the metrics file for the best model
        best_metrics = load_metrics(model_name_without_prefix_without_suffix)




        #save metrics
        save_metrics(f'{best_model_name}', best_metrics)


        # Save the best model using the constructed filename
        joblib.dump(best_model, f"{best_model_name}.pkl")
else:
    print("No best model found.")

import joblib
import pickle
import matplotlib.pyplot as plt
import numpy as np


print(best_model)
print(best_metrics)

# Check if best_metrics is not None
if best_metrics is not None:
    # Define the metrics and their corresponding values for the best model
    metrics = ['F1 Score', 'AUC ROC Score', 'Accuracy', 'Precision', 'Recall', 'Specificity', 'MCC']
    best_model_values = [
        best_metrics['Best F1 Score'],
        best_metrics['Best AUC ROC'],
        best_metrics['Best Accuracy'],
        best_metrics['Best Precision'],
        best_metrics['Best Recall'],
        best_metrics['Specificity'],
        best_metrics['MCC']
    ]

    # Create a radar chart for the best model
    angles = np.linspace(0, 2 * np.pi, len(metrics), endpoint=False).tolist()
    best_model_values += best_model_values[:1]  # Close the plot
    angles += angles[:1]  # Close the plot

    fig, ax = plt.subplots(figsize=(8, 8), subplot_kw={'polar': True})
    ax.fill(angles, best_model_values, 'b', alpha=0.1)

    ax.set_xticks(angles[:-1])
    ax.set_xticklabels(metrics)
    plt.title(f'Radar Chart for Best Model Metrics ({best_model})')
    plt.show()
else:
    print("No best metrics found.")

from sklearn.metrics import precision_score, recall_score

# Define a threshold for positive predictions (adjust as needed)
threshold = 0.5  # You may need to adjust this threshold

# Make predictions using the best model
y_pred_probabilities = best_model.predict_proba(X)[:, 1]

# Scenario (a): Calculate Recall for companies that will go bankrupt (success rate >= 60%)
success_rate_threshold_a = 0.6
y_true_a = (y >= success_rate_threshold_a).astype(int)
y_pred_a = (y_pred_probabilities >= threshold).astype(int)
recall_a = recall_score(y_true_a, y_pred_a)

# Scenario (b): Calculate Precision for companies that will not go bankrupt (success rate >= 70%)
success_rate_threshold_b = 0.7
y_true_b = (y < (1 - success_rate_threshold_b)).astype(int)
y_pred_b = (y_pred_probabilities < threshold).astype(int)
precision_b = precision_score(y_true_b, y_pred_b)

# Check if the conditions are satisfied
condition_a_satisfied = (recall_a >= 0.8)  # You can adjust the desired recall threshold here
condition_b_satisfied = (precision_b >= 0.8)  # You can adjust the desired precision threshold here

# Print results
print("Recall for Scenario (a):", recall_a)
print("Precision for Scenario (b):", precision_b)

# Print whether the conditions are satisfied or not
if condition_a_satisfied:
    print("Scenario (a) is satisfied.")
else:
    print("Scenario (a) is not satisfied.")

if condition_b_satisfied:
    print("Scenario (b) is satisfied.")
else:
    print("Scenario (b) is not satisfied.")

# Combine techniques and composite scores into pairs - NO STRATIFIED
composite_scores = {
    "KNN": 0.5 * best_f1_scores[0] + 0.5 * best_auc_roc_scores[0],
    "Naive Bayes": 0.5 * best_f1_scores[1] + 0.5 * best_auc_roc_scores[1],
    "Decision Trees": 0.5 * best_f1_scores[2] + 0.5 * best_auc_roc_scores[2],
    "Random Forest": 0.5 * best_f1_scores[3] + 0.5 * best_auc_roc_scores[3],
    "LDA": 0.5 * best_f1_scores[4] + 0.5 * best_auc_roc_scores[4],
    "SVM": 0.5 * best_f1_scores[5] + 0.5 * best_auc_roc_scores[5],
    "Logistic Reg": 0.5 * best_f1_scores[6] + 0.5 * best_auc_roc_scores[6]
}

# Sort techniques based on composite scores in descending order
sorted_techniques = sorted(composite_scores.keys(), key=lambda x: composite_scores[x], reverse=True)
sorted_composite_scores = [composite_scores[technique] for technique in sorted_techniques]

# Sort F1 scores based on techniques in descending order
sorted_f1_scores = [best_f1_scores[techniques.index(technique)] for technique in sorted_techniques]

# Create two subplots side by side
plt.figure(figsize=(12, 6))
plt.subplot(1, 2, 1)
bars = plt.bar(sorted_techniques, sorted_composite_scores, color='skyblue')
for bar, score in zip(bars, sorted_composite_scores):
    plt.text(bar.get_x() + bar.get_width() / 2, bar.get_height(), f'{score:.2f}', ha='center', va='bottom')
plt.title('Sorted Composite Scores')
# Rotate x-axis labels at a 45-degree angle for better readability
plt.xticks(rotation=45, ha='right')
plt.ylabel('Composite Score')

# SORTED BY COMPOSITE-NO STRATIFIED
plt.subplot(1, 2, 2)
bars_f1 = plt.bar(sorted_techniques, sorted_f1_scores, color='skyblue')
for bar, score in zip(bars_f1, sorted_f1_scores):
    plt.text(bar.get_x() + bar.get_width() / 2, bar.get_height(), f'{score:.2f}', ha='center', va='bottom')
plt.title('Sorted F1 Scores by Composite Score')
# Rotate x-axis labels at a 45-degree angle for better readability
plt.xticks(rotation=45, ha='right')
plt.ylabel('F1 Score')

plt.tight_layout()
plt.show()

# Sort AUC ROC scores based on techniques in descending order
sorted_auc_roc_scores = [best_auc_roc_scores[techniques.index(technique)] for technique in sorted_techniques]


plt.subplot(1, 2, 2)
bars_auc_roc = plt.bar(sorted_techniques, sorted_auc_roc_scores, color='goldenrod')
for bar, score in zip(bars_auc_roc, sorted_auc_roc_scores):
    plt.text(bar.get_x() + bar.get_width() / 2, bar.get_height(), f'{score:.2f}', ha='center', va='bottom')
plt.title('Sorted AUC ROC Scores by Composite Score')
# Rotate x-axis labels at a 45-degree angle for better readability
plt.xticks(rotation=45, ha='right')
plt.ylabel('AUC ROC Score')

plt.tight_layout()
plt.show()

#F1 AND AUC ROC SCORES -NO STRATIFIED

import matplotlib.pyplot as plt
import numpy as np

# Data
techniques = ["KNN", "Naive Bayes", "Decision Trees", "Random Forest","LDA","SVM","Logistic Reg"]
best_f1_scores = [knn_grid_search.best_score_, nb_grid_search.best_score_, dt_grid_search.best_score_, best_f1_score,\
                  lda_grid_search.best_score_, svm_grid_search.best_score_, logistic_grid_search.best_score_]
best_auc_roc_scores = [roc_auc_score(y, y_pred_knn), roc_auc_score(y, y_pred_nb), roc_auc_score(y, y_pred_dt), roc_auc_score(y, y_pred_rf),\
                       roc_auc_score(y, y_pred_lda), roc_auc_score(y, y_pred_svm),roc_auc_score(y, y_pred_logistic)]

# Create an array of indices for the techniques
x = np.arange(len(techniques))

# Bar width
width = 0.35

# Create a figure and axis
fig, ax = plt.subplots(figsize=(12, 6))

# Plot Best F1 Score
ax.bar(x - width/2, best_f1_scores, width, label='Best F1 Score', color='skyblue')
# Plot Best AUC ROC Score
ax.bar(x + width/2, best_auc_roc_scores, width, label='Best AUC ROC Score', color='goldenrod')

# Set axis labels and title
ax.set_xlabel('Techniques')
ax.set_ylabel('Score')
ax.set_title('Best F1 Score and Best AUC ROC Score by Technique')
ax.set_xticks(x)
ax.set_xticklabels(techniques)
ax.legend()

# Add text labels for F1 Score and AUC ROC Score inside the bars
for i in range(len(techniques)):
    ax.text(x[i] - width/2, best_f1_scores[i], f'{best_f1_scores[i]:.2f}', ha='center', va='bottom')
    ax.text(x[i] + width/2, best_auc_roc_scores[i], f'{best_auc_roc_scores[i]:.2f}', ha='center', va='bottom')

# Display the combined graph
plt.tight_layout()
plt.show()

import joblib
import pickle
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import StratifiedKFold, cross_val_score
from sklearn.metrics import make_scorer, f1_score, roc_auc_score
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis

# Define a dictionary to store the models
models = {
    "best_knn": joblib.load('knn_model.pkl'),
    "best_naive_bayes": joblib.load('naive_bayes_model.pkl'),
    "best_decision_tree": joblib.load('decision_tree_model.pkl'),
    "best_random_forest": joblib.load('random_forests_model.pkl'),
    "best_lda": joblib.load('lda_model.pkl'),
    "best_svm": joblib.load('svm_model.pkl'),
    "best_logistic": joblib.load('logistic_model.pkl'),

}

# Define the cross-validation strategy (e.g., Stratified K-Fold)
skf = StratifiedKFold(n_splits=4, shuffle=True, random_state=42)

# Initialize variables to track the best model and its score
best_model_name = None
best_composite_score = 0.0  # Initialize with a low score
best_metrics = None  # Initialize with None

# Define a function to calculate the composite score
def calculate_composite_score(f1_score, auc_roc_score, f1_weight=0.6, auc_roc_weight=0.4):
    return (f1_weight * f1_score) + (auc_roc_weight * auc_roc_score)

# Initialize a DataFrame to store the scores
score_data = pd.DataFrame(columns=['Model', 'F1 Score', 'AUC ROC Score', 'Composite Score'])

# Iterate through models and calculate their scores
for model_name, model in models.items():
    # Calculate the mean F1 Score using cross-validation
    f1_scores = cross_val_score(model, X, y, cv=skf, scoring=make_scorer(f1_score))
    mean_f1_score = f1_scores.mean()

    # Calculate the mean AUC ROC Score using cross-validation
    auc_roc_scores = cross_val_score(model, X, y, cv=skf, scoring=make_scorer(roc_auc_score))
    mean_auc_roc_score = auc_roc_scores.mean()

    # Calculate the composite score
    composite_score = calculate_composite_score(mean_f1_score, mean_auc_roc_score)

    # Append the scores to the DataFrame
    score_data = score_data.append([
        {'Model': model_name, 'Metric': 'F1 Score', 'Value': mean_f1_score},
        {'Model': model_name, 'Metric': 'AUC ROC Score', 'Value': mean_auc_roc_score},
        {'Model': model_name, 'Metric': 'Composite Score', 'Value': composite_score}
    ], ignore_index=True)

# Sort each metric's scores separately
sorted_data = pd.concat([
    score_data[score_data['Metric'] == 'Composite Score'].sort_values(by='Value', ascending=False),
    score_data[score_data['Metric'] == 'F1 Score'].sort_values(by='Value', ascending=False),
    score_data[score_data['Metric'] == 'AUC ROC Score'].sort_values(by='Value', ascending=False)
])

# Plotting the sorted scores
plt.figure(figsize=(12, 8))
sns.barplot(x='Model', y='Value', hue='Metric', data=sorted_data)
plt.title('Sorted Model Comparison')
plt.xticks(rotation=45)
plt.tight_layout()

# Annotating each bar with its value
ax = plt.gca()
for p in ax.patches:
    ax.annotate(format(p.get_height(), '.2f'),
                (p.get_x() + p.get_width() / 2., p.get_height()),
                ha='center', va='center',
                xytext=(0, 9),
                textcoords='offset points')

plt.tight_layout()

plt.show()

import joblib
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import StratifiedKFold, cross_val_score
from sklearn.metrics import make_scorer, f1_score, roc_auc_score
import joblib
import pickle
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import StratifiedKFold, cross_val_score
from sklearn.metrics import make_scorer, f1_score, roc_auc_score
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis

# Define the models
models = {
    "best_knn": joblib.load('knn_model.pkl'),
    "best_naive_bayes": joblib.load('naive_bayes_model.pkl'),
    "best_decision_tree": joblib.load('decision_tree_model.pkl'),
    "best_random_forest": joblib.load('random_forests_model.pkl'),
    "best_lda": joblib.load('lda_model.pkl'),
    "best_svm": joblib.load('svm_model.pkl'),
    "best_logistic": joblib.load('logistic_model.pkl'),

}

# Define the cross-validation strategy
skf = StratifiedKFold(n_splits=4, shuffle=True, random_state=42)

# Define a function to calculate the composite score
def calculate_composite_score(f1_score, auc_roc_score, f1_weight=0.6, auc_roc_weight=0.4):
    return (f1_weight * f1_score) + (auc_roc_weight * auc_roc_score)

# Initialize a DataFrame to store the scores
score_data = pd.DataFrame(columns=['Model', 'Metric', 'Value'])

# Iterate through models and calculate their scores
for model_name, model in models.items():
    # Calculate the mean F1 Score using cross-validation
    f1_scores = cross_val_score(model, X, y, cv=skf, scoring=make_scorer(f1_score))
    mean_f1_score = f1_scores.mean()

    # Calculate the mean AUC ROC Score using cross-validation
    auc_roc_scores = cross_val_score(model, X, y, cv=skf, scoring=make_scorer(roc_auc_score))
    mean_auc_roc_score = auc_roc_scores.mean()

    # Calculate the composite score
    composite_score = calculate_composite_score(mean_f1_score, mean_auc_roc_score)

    # Append the scores to the DataFrame
    score_data = score_data.append([
        {'Model': model_name, 'Metric': 'F1 Score', 'Value': mean_f1_score},
        {'Model': model_name, 'Metric': 'AUC ROC Score', 'Value': mean_auc_roc_score},
        {'Model': model_name, 'Metric': 'Composite Score', 'Value': composite_score}
    ], ignore_index=True)

# Sort each metric's scores separately
sorted_data = pd.concat([
    score_data[score_data['Metric'] == 'Composite Score'].sort_values(by='Value', ascending=False),
    score_data[score_data['Metric'] == 'F1 Score'].sort_values(by='Value', ascending=False),
    score_data[score_data['Metric'] == 'AUC ROC Score'].sort_values(by='Value', ascending=False)
])

# Improved plotting
plt.figure(figsize=(14, 10))
sns.set(style="whitegrid")  # Sets the style of the plot
palette = sns.color_palette("Paired")  # Change to your preferred palette

sns.barplot(x='Model', y='Value', hue='Metric', data=sorted_data, palette=palette)
plt.title('Sorted Model Comparison', fontsize=18)
plt.xlabel('Models', fontsize=14)
plt.ylabel('Score', fontsize=14)
plt.xticks(rotation=45, fontsize=12)
plt.yticks(fontsize=12)
plt.legend(title='Metric', title_fontsize='13', fontsize='12', loc='upper left', bbox_to_anchor=(1.05, 1), borderaxespad=0.)

# Enhanced annotations
ax = plt.gca()
for p in ax.patches:
    ax.annotate(format(p.get_height(), '.2f'),
                (p.get_x() + p.get_width() / 2., p.get_height()),
                ha='center', va='center',
                xytext=(0, 9),
                textcoords='offset points',
                fontsize=10)

plt.tight_layout()
plt.show()

#EXTRA WORK - ENSEMBLING
import joblib
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import StratifiedKFold, cross_val_score
from sklearn.metrics import make_scorer, f1_score, roc_auc_score
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import StackingClassifier, RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis

# Choose best  models according to the graph
knn_model = joblib.load('knn_model.pkl')  # Make sure the file path is correct
naive_bayes_model = joblib.load('naive_bayes_model.pkl')
decision_tree_model = joblib.load('decision_tree_model.pkl')





# Define the cross-validation strategy
skf = StratifiedKFold(n_splits=4, shuffle=True, random_state=42)

base_models = [
    ('knn', knn_model),
    ('naive_bayes', naive_bayes_model),
    ('decision_tree', decision_tree_model),
]

# Define the meta-model (logistic regression in this case)
meta_model = LogisticRegression()

# Create the stacking classifier
stacked_model = StackingClassifier(estimators=base_models, final_estimator=meta_model, cv=skf)

# Fit the stacked model
stacked_model.fit(X, y)

# Evaluate the stacked model using cross-validation
stacked_f1_scores = cross_val_score(stacked_model, X, y, cv=skf, scoring=make_scorer(f1_score))
stacked_auc_scores = cross_val_score(stacked_model, X, y, cv=skf, scoring=make_scorer(roc_auc_score))

# Initialize a DataFrame to store the scores
score_data = pd.DataFrame(columns=['Model', 'Metric', 'Value'])

# Append scores of individual models and the stacked model
model_names = ['KNN', 'Decision Tree', 'SVM', 'Stacked Model']
for model_name, model in zip(model_names, base_models + [('stacked', stacked_model)]):
    model_f1_scores = cross_val_score(model[1], X, y, cv=skf, scoring=make_scorer(f1_score))
    model_auc_scores = cross_val_score(model[1], X, y, cv=skf, scoring=make_scorer(roc_auc_score))

    score_data = score_data.append([
        {'Model': model_name, 'Metric': 'F1 Score', 'Value': model_f1_scores.mean()},
        {'Model': model_name, 'Metric': 'AUC ROC Score', 'Value': model_auc_scores.mean()}
    ], ignore_index=True)

# Plotting the scores
plt.figure(figsize=(14, 8))
sns.set(style="whitegrid")
palette = sns.color_palette("Paired")

sns.barplot(x='Model', y='Value', hue='Metric', data=score_data, palette=palette)
plt.title('Model Performance Comparison', fontsize=18)
plt.xlabel('Model', fontsize=14)
plt.ylabel('Score', fontsize=14)
plt.xticks(rotation=45, fontsize=12)
plt.yticks(fontsize=12)
plt.legend(title='Metric', title_fontsize='13', fontsize='12', loc='upper left')

# Enhanced annotations
ax = plt.gca()
for p in ax.patches:
    ax.annotate(format(p.get_height(), '.2f'),
                (p.get_x() + p.get_width() / 2., p.get_height()),
                ha='center', va='center',
                xytext=(0, 9),
                textcoords='offset points',
                fontsize=10)

plt.tight_layout()
plt.show()